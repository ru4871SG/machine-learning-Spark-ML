{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this section to suppress warnings generated by the code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1. Import the necessary libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2. Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Clustering with Spark ML\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length of kernel: double (nullable = true)\n",
      " |-- width of kernel: double (nullable = true)\n",
      " |-- asymmetry coefficient: double (nullable = true)\n",
      " |-- length of kernel groove: double (nullable = true)\n",
      "\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "## Step 3. Read the data from a CSV file\n",
    "sdf = spark.read.csv(\"sources/seeds.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# print the schema and the number of records\n",
    "sdf.printSchema()\n",
    "print(sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " area                    | 15.26  \n",
      " perimeter               | 14.84  \n",
      " compactness             | 0.871  \n",
      " length of kernel        | 5.763  \n",
      " width of kernel         | 3.312  \n",
      " asymmetry coefficient   | 2.221  \n",
      " length of kernel groove | 5.22   \n",
      "-RECORD 1-------------------------\n",
      " area                    | 14.88  \n",
      " perimeter               | 14.57  \n",
      " compactness             | 0.8811 \n",
      " length of kernel        | 5.554  \n",
      " width of kernel         | 3.333  \n",
      " asymmetry coefficient   | 1.018  \n",
      " length of kernel groove | 4.956  \n",
      "-RECORD 2-------------------------\n",
      " area                    | 14.29  \n",
      " perimeter               | 14.09  \n",
      " compactness             | 0.905  \n",
      " length of kernel        | 5.291  \n",
      " width of kernel         | 3.337  \n",
      " asymmetry coefficient   | 2.699  \n",
      " length of kernel groove | 4.825  \n",
      "-RECORD 3-------------------------\n",
      " area                    | 13.84  \n",
      " perimeter               | 13.94  \n",
      " compactness             | 0.8955 \n",
      " length of kernel        | 5.324  \n",
      " width of kernel         | 3.379  \n",
      " asymmetry coefficient   | 2.259  \n",
      " length of kernel groove | 4.805  \n",
      "-RECORD 4-------------------------\n",
      " area                    | 16.14  \n",
      " perimeter               | 14.99  \n",
      " compactness             | 0.9034 \n",
      " length of kernel        | 5.658  \n",
      " width of kernel         | 3.562  \n",
      " asymmetry coefficient   | 1.355  \n",
      " length of kernel groove | 5.175  \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 4. Data Preview - Show the top 5 rows\n",
    "sdf.show(n=5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------------+---------------+---------------------+-----------------------+\n",
      "|area|perimeter|compactness|length of kernel|width of kernel|asymmetry coefficient|length of kernel groove|\n",
      "+----+---------+-----------+----------------+---------------+---------------------+-----------------------+\n",
      "|null|     null|     0.8099|            null|          2.641|                 null|                  5.185|\n",
      "|12.2|     null|     0.8874|            null|           null|                 null|                    5.0|\n",
      "|12.3|    13.34|       null|            null|           null|                5.637|                   null|\n",
      "+----+---------+-----------+----------------+---------------+---------------------+-----------------------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 5. Use Spark SQL to check if there are any null values\n",
    "# Get the list of all columns\n",
    "columns = sdf.columns\n",
    "\n",
    "# Create a temporary view\n",
    "sdf.createOrReplaceTempView(\"seeds\")\n",
    "\n",
    "# Find and show rows with any null values\n",
    "rows_with_nulls = spark.sql(\"SELECT * FROM seeds WHERE \" + \" OR \".join([f\"`{col}` IS NULL\" for col in columns]))\n",
    "rows_with_nulls.show()\n",
    "\n",
    "# Count the number of rows with any null values\n",
    "count_rows_with_nulls = spark.sql(\"SELECT COUNT(*) FROM seeds WHERE \" + \" OR \".join([f\"`{col}` IS NULL\" for col in columns]))\n",
    "count_rows_with_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6. Data Cleaning\n",
    "# Previous step showed that there are several rows with many null values, so we need to remove them\n",
    "sdf = sdf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "## Step 7. Verify Data Cleaning\n",
    "# Check the number of records after removing the rows with null values (should be 210)\n",
    "print(sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8. Assemble all columns into a single vector\n",
    "feature_cols = columns\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "sdf_transformed = assembler.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9. Create 4 Clusters\n",
    "number_of_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 10. Create a K-Means clustering model\n",
    "kmeans = KMeans(k = number_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 11. Train the model on the transformed data\n",
    "model = kmeans.fit(sdf_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 12. Make predictions on the dataset\n",
    "predictions = model.transform(sdf_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "| area|perimeter|compactness|length of kernel|width of kernel|asymmetry coefficient|length of kernel groove|            features|prediction|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "|15.26|    14.84|      0.871|           5.763|          3.312|                2.221|                   5.22|[15.26,14.84,0.87...|         0|\n",
      "|14.88|    14.57|     0.8811|           5.554|          3.333|                1.018|                  4.956|[14.88,14.57,0.88...|         0|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "| area|perimeter|compactness|length of kernel|width of kernel|asymmetry coefficient|length of kernel groove|            features|prediction|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "|13.99|    13.83|     0.9183|           5.119|          3.383|                5.234|                  4.781|[13.99,13.83,0.91...|         1|\n",
      "|14.28|    14.17|     0.8944|           5.397|          3.298|                6.685|                  5.001|[14.28,14.17,0.89...|         1|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "| area|perimeter|compactness|length of kernel|width of kernel|asymmetry coefficient|length of kernel groove|            features|prediction|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "|17.63|    15.98|     0.8673|           6.191|          3.561|                4.076|                   6.06|[17.63,15.98,0.86...|         2|\n",
      "|16.84|    15.67|     0.8623|           5.998|          3.484|                4.675|                  5.877|[16.84,15.67,0.86...|         2|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "| area|perimeter|compactness|length of kernel|width of kernel|asymmetry coefficient|length of kernel groove|            features|prediction|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "|13.84|    13.94|     0.8955|           5.324|          3.379|                2.259|                  4.805|[13.84,13.94,0.89...|         3|\n",
      "|13.89|    14.02|      0.888|           5.439|          3.199|                3.986|                  4.738|[13.89,14.02,0.88...|         3|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+-----------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 13. Use Spark SQL to display the prediction results\n",
    "# Create a temporary view\n",
    "predictions.createOrReplaceTempView(\"seeds_predictions\")\n",
    "\n",
    "# Check two rows from each cluster\n",
    "spark.sql(\"SELECT * FROM seeds_predictions WHERE prediction = 0 LIMIT 2\").show()\n",
    "spark.sql(\"SELECT * FROM seeds_predictions WHERE prediction = 1 LIMIT 2\").show()\n",
    "spark.sql(\"SELECT * FROM seeds_predictions WHERE prediction = 2 LIMIT 2\").show()\n",
    "spark.sql(\"SELECT * FROM seeds_predictions WHERE prediction = 3 LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   56|\n",
      "|         3|   39|\n",
      "|         2|   55|\n",
      "|         0|   60|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 14. Display how many records are in each cluster\n",
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 15. Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
